{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_and_run.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKJWdJqfdCNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1touOLydHNd",
        "colab_type": "text"
      },
      "source": [
        "#import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjNFC9LvdIH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import time as t\n",
        "\n",
        "def get_dataset(resol_max,path, validation = False, database = 'cifar10', download = True):\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize(size=(resol_max, resol_max)), transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "    if validation:\n",
        "        raise NotImplementedError\n",
        "    if database=='cifar10':\n",
        "        trainset = torchvision.datasets.CIFAR10(root= path, train=True, transform=transform, download=download)\n",
        "        testset = torchvision.datasets.CIFAR10(root = path, train=False,transform=transform, download=download)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return trainset, testset\n",
        "\n",
        "\n",
        "def get_loader(data, batch_size, num_workers=2):\n",
        "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True,num_workers = num_workers)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def accuracy(net,testset,batch_size = 64,cuda=True):\n",
        "    r=net.resolution\n",
        "    testloader = get_loader(testset, batch_size)\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if cuda:\n",
        "        net.to('cuda')\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            if cuda:\n",
        "                images = images.to('cuda')\n",
        "                labels = labels.to('cuda')\n",
        "            ## dans tous les cas ici il faut changer la resol de l'image \n",
        "            images = F.interpolate(images, r)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "           \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        return 100.0 * correct/total\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(model, optimizer, trainset, loss_fn, valset=None, batch_size=64,  n_epoch = 5, cuda=True, disp_stats = True, validate = False):\n",
        "    if cuda:\n",
        "        model.to('cuda')\n",
        "\n",
        "    if validate :\n",
        "        valloader = get_loader(valset, batch_size)\n",
        "\n",
        "\n",
        "    resol_cible = model.resolution\n",
        "    trainloader = get_loader(trainset, batch_size)\n",
        "\n",
        "    loss_train, loss_val = [], []\n",
        "    acc_train, acc_val = [],[]\n",
        "\n",
        "    loss_func = loss_fn()\n",
        "    \n",
        "    for epoch in range(n_epoch):\n",
        "        running_trainloss = 0\n",
        "        for data in trainloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs = inputs.to('cuda') \n",
        "                labels = labels.to('cuda')\n",
        "\n",
        "            # on change la resol de l'image    \n",
        "            optimizer.zero_grad()\n",
        "            inputs = F.interpolate(inputs, resol_cible)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, labels)\n",
        "            loss.backward()\n",
        "            running_trainloss+= loss\n",
        "            optimizer.step()\n",
        "    \n",
        "        # maintenant plus qu'a calculer les stats et les afficher\n",
        "        loss_train.append(running_trainloss)\n",
        "        acct = accuracy(model, trainset, batch_size=batch_size)\n",
        "        acc_train.append(acct)\n",
        "        if disp_stats:\n",
        "            print('Epoch', epoch, '/', n_epoch, ':')\n",
        "            print('train loss : ', running_trainloss)\n",
        "            print('train acc',acct )\n",
        "\n",
        "        # stats de validation\n",
        "        if validate:\n",
        "            vloss = 0\n",
        "            for data in valloader:\n",
        "                ipt, lbl = data\n",
        "                if cuda : \n",
        "                    ipt = ipt.to('cuda')\n",
        "                    lbl = ipt.to('cuda')\n",
        "                ipt = F.interpolate(ipt, resol_cible)\n",
        "                out = model(ipt)\n",
        "                vloss+= loss_func(out, lbl)\n",
        "            loss_val.append(vloss)\n",
        "            accv = accuracy(model, valset, batch_size)\n",
        "            acc_val.append(accv)\n",
        "            if disp_stats:\n",
        "                print('val loss : ', vloss)\n",
        "                print('val acc' ,accv)\n",
        "\n",
        "    if validate:\n",
        "        raise NotImplementedError\n",
        "    else:\n",
        "        return model, loss_train, acc_train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "      super(ResNetBlock, self).__init__()\n",
        "      self.stride = stride\n",
        "      self.in_planes=in_planes\n",
        "      self.planes = planes\n",
        "      if stride!=1:\n",
        "\n",
        "        self.fx = nn.Sequential(nn.Conv2d(in_planes, planes, 3, stride=2, \n",
        "                                          padding=1),\n",
        "                                nn.ReLU(), \n",
        "                                nn.Conv2d(planes, planes,3, padding=1))\n",
        "        \n",
        "        self.iden = nn.Conv2d(self.in_planes, self.planes, 3, stride = 2,\n",
        "                              padding =1)\n",
        "\n",
        "      else:\n",
        "        self.fx = nn.Sequential(nn.Conv2d(planes, planes, 3, padding = 1),\n",
        "                                nn.ReLU(), \n",
        "                                nn.Conv2d(planes, planes,3, padding=1))\n",
        "        self.iden = nn.Sequential()\n",
        "          \n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    if self.stride ==1:\n",
        "      fx = self.fx(x)\n",
        "      if fx.size()!=self.iden(x).size():\n",
        "        print(fx.size(),self.iden(x).size() )\n",
        "        print(\"1\")\n",
        "      \n",
        "      out = fx + self.iden(x)\n",
        "      return F.relu(out)\n",
        "\n",
        "    else:\n",
        "\n",
        "      fx = self.fx(x)\n",
        "      if fx.size()!=self.iden(x).size():\n",
        "        print(fx.size(),self.iden(x).size() )\n",
        "        print(\"2\")\n",
        "    \n",
        "      out = fx + self.iden(x)\n",
        "      return F.relu(out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, depth,resolution,width=16, num_classes=10,input_dim=3,Block = ResNetBlock):\n",
        "      super(ResNet, self).__init__()\n",
        "      self.in_planes = width\n",
        "      self.conv1 = nn.Conv2d(input_dim, width, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "      self.bn1 = nn.BatchNorm2d(width)\n",
        "      self.depth = depth\n",
        "      self.width = width\n",
        "      self.resolution = resolution\n",
        "      \n",
        "      resol = resolution\n",
        "      # construction des piles\n",
        "      plane = width\n",
        "      layers = []\n",
        "      for nb in depth:\n",
        "        layer = self._make_layer(Block,plane ,nb,2)\n",
        "        layers.append(layer)\n",
        "        plane*=2\n",
        "        resol/=2\n",
        "      \n",
        "      self.layers = nn.Sequential(*layers)\n",
        "      # arrivé ici on en est à plane@resol*resol\n",
        "      # on applique le pooling\n",
        "      self.pool = nn.AvgPool2d(4)\n",
        "      resol = int(resol/4)\n",
        "\n",
        "      out_size = resol*resol*plane \n",
        "      self.linear = nn.Linear(out_size, num_classes)\n",
        "\n",
        "  def _make_layer(self, Block, planes, num_blocks, stride):\n",
        "      layers = []\n",
        "      block1 = Block(planes, 2*planes, stride = 2)\n",
        "      planes *=2\n",
        "      layers.append(block1)\n",
        "      for i in range(1,num_blocks):\n",
        "        block_new = Block(planes, planes, stride =1)\n",
        "        layers.append(block_new)\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "      \n",
        "    # premiere etape\n",
        "      out = F.relu(self.bn1(self.conv1(x)))\n",
        "     \n",
        "      # passage par les piles \n",
        "      out = self.layers(out)\n",
        "      \n",
        "      # output\n",
        "      out = self.pool(out)\n",
        "      out = out.view(out.size(0), -1)\n",
        "      out = self.linear(out)\n",
        "      return out\n",
        "\n",
        "def get_resnet(depth = [2,2,2,2],resolution = 224,width=16, num_classes=10,input_dim=3):\n",
        "  resnet = ResNet(depth,resolution,width,num_classes=10,input_dim=3)\n",
        "  return resnet\n",
        "\n",
        "def get_new_depth(alpha, depth):\n",
        "  old_sum_depth = sum(depth)\n",
        "  new_sum_depth = int(alpha*old_sum_depth)\n",
        "  new_depth = depth \n",
        "  new_sum = old_sum_depth\n",
        "  for i in range(len(depth)):\n",
        "    if new_sum < new_sum_depth:\n",
        "      new_depth[i]+=1\n",
        "      new_sum +=1\n",
        "  return new_depth\n",
        "\n",
        "\n",
        "def gridsearch(trainset, testset,loss_fn,precision=.2,epsilon = .5,phi=1, bs=64,learning_rate=1e-2,n_epoch=2):\n",
        "  solu=1,1,1\n",
        "  depth, width, resol = [2,2,2,2], 16, 112  ## ce sont les valeurs de base à scaler ensuite\n",
        "  # training\n",
        "  print('entrainement modele 1')\n",
        "  model1 = get_resnet(depth, resol, width)\n",
        "  optimizer = torch.optim.Adam(model1.parameters(),lr=learning_rate)\n",
        "  debut = t.time()\n",
        "  model1, _, _ =train(model1, optimizer, trainset, loss_fn, batch_size=bs, n_epoch=n_epoch, disp_stats=False, validate=False)\n",
        "  fin = t.time()\n",
        "  print('temps d entrainement : ', fin-debut)\n",
        "  # testing\n",
        "  accu_max=accuracy(model1, testset, batch_size=bs)\n",
        "\n",
        "  for alpha in np.arange(1,2,precision):\n",
        "    for beta in np.arange(1,2,precision):\n",
        "      for gamma in np.arange(1,2,precision):\n",
        "        if np.abs(alpha*beta**2*gamma**2 - 2)<=epsilon:\n",
        "          # il faut réentraîner le modele\n",
        "          print('entrainement avec parametres', alpha, beta, gamma)\n",
        "          new_depth = get_new_depth(alpha, depth)\n",
        "          \n",
        "          print(\"width : \", int(beta*width))\n",
        "          print(\"resol : \", int(gamma*resol))\n",
        "          print(\"depth: \",  new_depth)\n",
        "          model = get_resnet(depth = new_depth, width = int(beta*width), resolution = int(gamma*resol))\n",
        "\n",
        "          optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "          deb = t.time()\n",
        "          model, _, _ = train(model, optimizer, trainset, loss_fn, batch_size=bs, n_epoch=n_epoch, disp_stats=False, validate=False)\n",
        "          fin = t.time()\n",
        "          print(\"temps d entrainement : \", fin-deb)\n",
        "          accu = accuracy(model, testset, batch_size = bs)\n",
        "          if accu>accu_max:\n",
        "            accu_max= accu\n",
        "            solu=alpha,beta,gamma\n",
        "    return solu\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GAP5xbqdNuq",
        "colab_type": "text"
      },
      "source": [
        "# tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ8l7Q8FdOfa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6aed94d-f017-40b2-c20e-a79db1bffb5e"
      },
      "source": [
        "trainset, testset = get_dataset(224, '/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usy_0cP9f_Ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUUkMfJojckq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pour l'entrainement c'est environ 4 min par epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hMQDp-1ftrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e98c494f-5319-4421-e587-14ec9a1c0a63"
      },
      "source": [
        "gridsearch(trainset, testset, loss_fn, n_epoch = 1, bs=220)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entrainement modele 1\n",
            "temps d entrainement :  200.72062635421753\n",
            "entrainement avec parametres 1.0 1.0 1.4\n",
            "width :  16\n",
            "resol :  156\n",
            "depth:  [2, 2, 2, 2]\n",
            "temps d entrainement :  437.00736904144287\n",
            "entrainement avec parametres 1.0 1.2 1.2\n",
            "width :  19\n",
            "resol :  134\n",
            "depth:  [2, 2, 2, 2]\n",
            "temps d entrainement :  674.967503786087\n",
            "entrainement avec parametres 1.0 1.4 1.0\n",
            "width :  22\n",
            "resol :  112\n",
            "depth:  [2, 2, 2, 2]\n",
            "temps d entrainement :  909.3671100139618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEshRJAjgYhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}