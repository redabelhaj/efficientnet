{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(path):\n",
    "    return torchvision.datasets.ImageFolder(\n",
    "        root=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_buff = '/Users/redabelhaj/Downloads/africa_dataset/only_buff'\n",
    "path_zebra = '/Users/redabelhaj/Downloads/africa_dataset/only_zebra'\n",
    "path_rhino = '/Users/redabelhaj/Downloads/africa_dataset/only_rhino'\n",
    "path_elephant = '/Users/redabelhaj/Downloads/africa_dataset/only_elephant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_buff_2 = path_buff + '/buffalo/'\n",
    "path_zebra_2 = path_zebra + '/zebra/'\n",
    "path_rhino_2 = path_rhino + '/rhino/'\n",
    "path_elephant_2 = path_elephant + '/elephant/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffalos = dataset(path_buff)\n",
    "zebras = dataset(path_zebra)\n",
    "rhinos = dataset(path_rhino)\n",
    "elephants = dataset(path_elephant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffalos_imgs = [buffalos[i][0] for i in range(len(buffalos))]\n",
    "zebras_imgs = [zebras[i][0] for i in range(len(zebras))]\n",
    "rhinos_imgs = [rhinos[i][0] for i in range(len(rhinos))]\n",
    "elephants_imgs = [elephants[i][0] for i in range(len(elephants))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(path):\n",
    "    list_exts = os.listdir(path)\n",
    "    names = []\n",
    "    for n in list_exts:\n",
    "        if n[-1]=='t':\n",
    "            names.append(path+n)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rectangles(path):\n",
    "    file_paths = get_file_paths(path)\n",
    "    numbers = [int(path[-7]+path[-6]+path[-5]) for path in file_paths]\n",
    "    n = max(numbers)\n",
    "    d_rectangles = {i : [] for i in range(1+n)}\n",
    "    for path in file_paths:\n",
    "        number = int(path[-7]+path[-6]+path[-5])\n",
    "        #print(number)\n",
    "        with open(path, \"r\") as file:\n",
    "            for line in file.readlines():\n",
    "                l = line.split(' ')\n",
    "                coords = float(l[1]), float(l[2]), float(l[3]), float(l[4].strip())\n",
    "                d_rectangles[number-1].append(coords)\n",
    "    return d_rectangles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rectangles_labels(path):\n",
    "    file_paths = get_file_paths(path)\n",
    "    numbers = [int(path[-7]+path[-6]+path[-5]) for path in file_paths]\n",
    "    n = max(numbers)\n",
    "    d_rectangles = {i : [] for i in range(1+n)}\n",
    "    for path in file_paths:\n",
    "        number = int(path[-7]+path[-6]+path[-5])\n",
    "        #print(number)\n",
    "        with open(path, \"r\") as file:\n",
    "            for line in file.readlines():\n",
    "                l = line.split(' ')\n",
    "                coords = float(l[1]), float(l[2]), float(l[3]), float(l[4].strip())\n",
    "                label = int(l[0])\n",
    "                d_rectangles[number-1].append((label, coords))\n",
    "    return d_rectangles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(images, path):\n",
    "    N = len(images)\n",
    "    d_rectangles = get_rectangles(path)\n",
    "    #print(d_rectangles)\n",
    "    res = []\n",
    "    for i in range(N):\n",
    "        rectangles = d_rectangles[i]\n",
    "        for rect in rectangles:\n",
    "            x,y,w_r,h_r = rect\n",
    "            w,h = images[i].size\n",
    "            x_abs = w*x\n",
    "            y_abs = h*y\n",
    "            bottom = y_abs + h_r*h/2\n",
    "            top = y_abs - h_r*h/2\n",
    "            left = x_abs - w_r*w/2\n",
    "            right = x_abs + w_r*w/2\n",
    "            box_crop= (left, top, right, bottom)\n",
    "            cropped_img =images[i].crop(box_crop)\n",
    "            res.append(cropped_img)\n",
    "    return res\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection_dataset(images, rect_labels):\n",
    "    res = []\n",
    "    for i in range(len(images)):\n",
    "        im = images[i]\n",
    "        list_rect_lab = rect_labels[i]\n",
    "        res.append((im, list_rect_lab))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_labels = get_rectangles_labels(path_buff_2)\n",
    "ds_buf = get_detection_dataset(buffalos_imgs, rect_labels)\n",
    "\n",
    "rect_labels = get_rectangles_labels(path_zebra_2)\n",
    "ds_zeb = get_detection_dataset(zebras_imgs, rect_labels)\n",
    "\n",
    "rect_labels = get_rectangles_labels(path_rhino_2)\n",
    "ds_r = get_detection_dataset(rhinos_imgs, rect_labels)\n",
    "\n",
    "rect_labels = get_rectangles_labels(path_elephant_2)\n",
    "ds_el = get_detection_dataset(elephants_imgs, rect_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_detect_tot = ds_buf + ds_zeb+ds_r + ds_el\n",
    "ds_detect = r.sample(ds_detect_tot, len(ds_detect_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=1379x1034 at 0x175710390>,\n",
       " [(2, (0.465625, 0.539906, 0.925, 0.920188))])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_detect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torchvision.transforms.ToTensor()\n",
    "ds_detect_t = [(interpolate(t(image),140), label) for (image, label) in ds_detect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-5aabe8c1cdef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuf_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffalos_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_buff_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-fe7c4dc2f99d>\u001b[0m in \u001b[0;36mget_images\u001b[0;34m(images, path)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_abs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_r\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mbox_crop\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mcropped_img\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mcrop\u001b[0;34m(self, box)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_crop\u001b[0;34m(self, im, box)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \u001b[0m_decompression_bomb_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsolute_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "buf_images = get_images(buffalos_imgs,path_buff_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_images = get_images(zebras_imgs,path_zebra_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_images = get_images(rhinos_imgs,path_rhino_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_images =  get_images(elephants_imgs,path_elephant_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pil_dataset(b,z,r,e):\n",
    "    res = []\n",
    "    for img in b:\n",
    "        res.append((img, 0))\n",
    "    for img in z:\n",
    "        res.append((img, 1))\n",
    "    for img in r:\n",
    "        res.append((img, 2))\n",
    "    for img in e:\n",
    "        res.append((img, 3))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_pil_dataset(buf_images, z_images, r_images, e_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(img, res):\n",
    "    if img.ndim ==4:\n",
    "        img_perm = F.interpolate(img, size = res)\n",
    "        img_perm = img_perm.permute(0,1,3,2)\n",
    "        img_perm = F.interpolate(img_perm, size =res)\n",
    "        img_perm = img_perm.permute(0,1,3,2)\n",
    "        return img_perm\n",
    "    elif img.ndim==3:\n",
    "        img_perm = F.interpolate(img, size = res)\n",
    "        img_perm = img_perm.permute(0,2,1)\n",
    "        img_perm = F.interpolate(img_perm, size =res)\n",
    "        img_perm = img_perm.permute(0,2,1)\n",
    "        return img_perm\n",
    "    else:\n",
    "        raise Exception(\"tensor dimension should be 3 or 4\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, res):\n",
    "    t = torchvision.transforms.ToTensor()\n",
    "    trans_data = [(interpolate(t(image),res), label) for (image, label) in data]\n",
    "    return trans_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuf_ds =  r.sample(dataset, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = shuf_ds[:2000]\n",
    "testset = shuf_ds[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = transform(trainset, 100)\n",
    "testset = transform(testset, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import *\n",
    "from resnetclass import *\n",
    "from gridsearch import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = get_resnet(width = 17, resolution=140,depth=[3,3,2,2], num_classes = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/redabelhaj/Desktop/INF473V/Projet/models/resnetaf.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.55156950672645"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(resnet, testset, num_points = len(testset), cuda = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection with R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = requests.get(\"http://farm9.staticflickr.com/8445/7751093276_740e66ae99_z.jpg\")\n",
    "#image1 = open(\"image1.jpg\", \"wb\")\n",
    "#image1.write(response.content)\n",
    "#image1.close()\n",
    "\n",
    "#response = requests.get(\"http://farm1.staticflickr.com/138/358863752_c83b367fef_z.jpg\")\n",
    "#image2 = open(\"image2.jpg\", \"wb\")\n",
    "#image2.write(response.content)\n",
    "#image2.close()\n",
    "\n",
    "#response = requests.get(\"http://farm4.staticflickr.com/3624/3342769458_3bc41b3cce_z.jpg\")\n",
    "#image3 = open(\"image3.jpg\", \"wb\")\n",
    "#image3.write(response.content)\n",
    "#image3.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image1 = cv2.cvtColor(cv2.imread(\"image1.jpg\"),cv2.COLOR_BGR2RGB)\n",
    "#image2 = cv2.cvtColor(cv2.imread(\"image2.jpg\"),cv2.COLOR_BGR2RGB)\n",
    "#image3 = cv2.cvtColor(cv2.imread(\"image3.jpg\"),cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = selectivesearch(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectivesearch(img):\n",
    "  \"\"\"\n",
    "  SelectiveSearch : \n",
    "  - image au format openCV  \n",
    "  - renvoie une liste de quadruplets (x,y,w,h) représentant les zones de \n",
    "    prédiction\n",
    "  \"\"\"\n",
    "  ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "  ss.setBaseImage(img)\n",
    "  # Deux modes de détections sont possibles : Fast et Quality\n",
    "  #ss.switchToSelectiveSearchFast()\n",
    "  ss.switchToSelectiveSearchQuality()\n",
    "  return ss.process()\n",
    "\n",
    "def showrects(img,rects,maxRect=3000):\n",
    "  \"\"\"\n",
    "  showrects : affiche l'image img en y dessinant les rectangles énumérés dans \n",
    "  la liste rects.\n",
    "  \"\"\"\n",
    "  # On crée une copie de l'image\n",
    "  imOut = img.copy()\n",
    "  # Pour chaque rectangle proposé, si il est dans les 1000 plus probables,\n",
    "  # on le dessine sur l'image\n",
    "  for i,rect in enumerate(rects):\n",
    "      if (i < maxRect):\n",
    "          x, y, w, h = rect\n",
    "          cv2.rectangle(imOut, (x, y), (x+w, y+h), (0, 0, 255), 1, cv2.LINE_AA)\n",
    "  plt.imshow(imOut)\n",
    "\n",
    "def filter_rects(rects, min_size=30, max_size = 400):\n",
    "  \"\"\" \n",
    "  filter_rects: prend en argument une liste de rectangles et renvoie une liste contenant uniquement\n",
    "  ceux dont les côtés sont de taille comprise entre min_size et max_size.\n",
    "  \"\"\"\n",
    "  new_list = []\n",
    "\n",
    "  for rect in rects:\n",
    "    _,_,w,h = rect \n",
    "    if w >= min_size and w <= max_size and h >= min_size and h <= max_size :\n",
    "      new_list.append(rect)\n",
    "  return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation de l'image au format 224x224 attendu\n",
    "# par le modèle\n",
    "resize = transforms.Compose([\n",
    "                                 transforms.ToPILImage(),\n",
    "                                 transforms.Resize(140),\n",
    "                             transforms.CenterCrop(140)\n",
    "])\n",
    "# normalisation attendue par le modèle\n",
    "transform = transforms.Compose([\n",
    "      resize, \n",
    "      transforms.ToTensor()\n",
    "])\n",
    "# test de la transformation sur une image exemple.\n",
    "#image1 =transform(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_boxes(img,maxRect=2000,threshold=.5,iou=0.5):\n",
    "  \"\"\" \n",
    "  get_labeled_boxes : prend en argument\n",
    "    - une image (img),\n",
    "    - un nombre maximum de zones de prédictions à tester (maxRect),\n",
    "    - un seuil d'acceptation d'une zone de prédiction (threshold)\n",
    "    - un iou\n",
    "  \"\"\"\n",
    "  predictions = []\n",
    "  rects = selectivesearch(img)\n",
    "  rects = filter_rects(rects)\n",
    "  m = min(maxRect, len(rects))\n",
    "  for i in tqdm.notebook.tqdm(range(m)):\n",
    "    # crop l'image avec rects[i]\n",
    "    (x,y,w,h) = rects[i]\n",
    "    img_rect = img[y:y+h,x:x+w]\n",
    "    # on transforme l'image d'abord et on la resize \n",
    "    img_rect = transform(img_rect)\n",
    "    img_rect= img_rect.resize(1,3,140,140)\n",
    "\n",
    "\n",
    "    res_clf = resnet(img_rect)\n",
    "\n",
    "    ## on convertit en probas avec softmax\n",
    "    probs = torch.nn.functional.softmax(res_clf)\n",
    "    proba_max, ind  = torch.max(probs), torch.argmax(probs)\n",
    "\n",
    "    if proba_max > threshold:\n",
    "      # on conserve cette prédiction\n",
    "      triplet = rects[i], int(ind), float(proba_max)\n",
    "      predictions.append(triplet)\n",
    "\n",
    "  return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c32f94581344c01b342a667baf049d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/redabelhaj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#preds = get_labeled_boxes(image1,maxRect=2000,threshold=.9,iou=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = {0 : \"buffalo\", 1 : \"zebra\", 2:'rhino', 3:'elephant'}\n",
    "\n",
    "\n",
    "def show_objs(img,labeled_regions):\n",
    "  \"\"\" \n",
    "  - img : image\n",
    "  - labeled_regions : triplets (rect,label,score) :\n",
    "    - rect : quadruplet (x,y,w,h)\n",
    "    - label : un entier entre 0 et 1000\n",
    "    - score : un flottant entre 0 et 1\n",
    "  \"\"\"\n",
    "  imOut = img.copy()\n",
    "  \n",
    "  for rect,label,score in labeled_regions:\n",
    "      x, y, w, h = rect\n",
    "      cv2.rectangle(imOut, (x, y), (x+w, y+h), (0, 0, 255), 1, cv2.LINE_AA)\n",
    "      cv2.putText(imOut, \"{} : {:.2f}\".format(labels[label],score), (x,y+10),\n",
    "                  cv2.FONT_HERSHEY_SIMPLEX,.6,(0,255,255),2,cv2.LINE_AA)\n",
    "    \n",
    "  plt.imshow(imOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_objs(image1, preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(rect1,rect2):\n",
    "  \"\"\" \n",
    "  Renvoie un float avec le IoU entre rect1 et rect2 :\n",
    "  rect1, rect2 : quadruplets (x,y,w,h) \n",
    "  \"\"\"\n",
    "  x1,y1,w1,h1 = rect1\n",
    "  x2,y2,w2,h2 = rect2\n",
    "  xA,yA = max(x1,x2),max(y1,y2)\n",
    "  xB,yB = min(x1+w1,x2+w2),min(y1+h1,y2+h2)\n",
    "  inter = max(0,xB - xA)*max(0,yB-yA)\n",
    "  union = w1*h1 + w2*h2 - inter\n",
    "  return inter/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_boxes2(img,maxRect=2000,threshold=.99,iou=0.2):\n",
    "  \"\"\" \n",
    "  get_labeled_boxes : prend en argument\n",
    "    - une image (img),\n",
    "    - un nombre maximum de zones de prédictions à tester (maxRect),\n",
    "    - un seuil d'acceptation d'une zone de prédiction (threshold)\n",
    "    - un iou\n",
    "  \"\"\"\n",
    "  predictions = []\n",
    "  rects = selectivesearch(img)\n",
    "  rects = filter_rects(rects)\n",
    "  m = min(maxRect, len(rects))\n",
    "  for i in tqdm.notebook.tqdm(range(m)):\n",
    "    # crop l'image avec rects[i]\n",
    "    (x,y,w,h) = rects[i]\n",
    "    img_rect = img[y:y+h,x:x+w]\n",
    "    # on transforme l'image d'abord et on la resize \n",
    "    img_rect = transform(img_rect)\n",
    "    img_rect= img_rect.resize(1,3,140,140)\n",
    "    res_clf = resnet(img_rect)\n",
    "    ## on convertit en probas avec softmax\n",
    "    probs = torch.nn.functional.softmax(res_clf)\n",
    "    proba_max, ind  = torch.max(probs), torch.argmax(probs)\n",
    "    if proba_max > threshold:\n",
    "      # on conserve cette prédiction\n",
    "      triplet = rects[i], int(ind), float(proba_max)\n",
    "\n",
    "      #on va chercher dans les prédiction déja faites \n",
    "\n",
    "      ajouter = True \n",
    "      for tpt in predictions:\n",
    "        rct, idx, _ = tpt\n",
    "        if idx==int(ind) and IoU(rct, rects[i]) > iou:\n",
    "          # on a deja détecté ce rectagle\n",
    "          ajouter = False\n",
    "      if ajouter :\n",
    "        predictions.append(triplet) \n",
    "\n",
    "  return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a969d83c12e4d89bae2a0f488bba106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/redabelhaj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#labeled_regions2 = get_labeled_boxes2(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rcnn(nn.Module):\n",
    "    def __init__(self, resnet, num_classes):\n",
    "        super(Rcnn, self).__init__()\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.cnn = resnet.layers\n",
    "        self.num_classes = num_classes\n",
    "        # choper la taille de la feature map\n",
    "        r,d,w = resnet.resolution,resnet.depth,resnet.width\n",
    "        w_actu,r_actu = w,r\n",
    "        for nb in resnet.depth:\n",
    "            w_actu*=2\n",
    "            r_actu=roundsp(r_actu)\n",
    "        # taille de la feature map : w_actu@r_actu*r_actu\n",
    "        out_size = int(w_actu*r_actu*r_actu)\n",
    "        #print(out_size)\n",
    "        self.classifier = nn.Linear(out_size, num_classes+1)\n",
    "        self.bboxreg = nn.Linear(out_size, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.cnn(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        clf = self.classifier(out)\n",
    "        \n",
    "        bbox = self.bboxreg(out)\n",
    "        bbox = F.sigmoid(bbox) ## les 4 nombres seront entre 0 et 1\n",
    "        return clf, bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rcnn(resnet, num_classes, bn1_dict, conv1_dict, cnn_dict):\n",
    "    \"\"\" renvoie un rcnn avec cnn loadé et bloqué\"\"\"\n",
    "    model = Rcnn(resnet, num_classes)\n",
    "    model.bn1.load_state_dict(bn1_dict)\n",
    "    model.conv1.load_state_dict(conv1_dict)\n",
    "    model.cnn.load_state_dict(cnn_dict)\n",
    "    \n",
    "    for p in model.bn1.parameters():\n",
    "        p.requires_grad=False\n",
    "    for p in model.conv1.parameters():\n",
    "        p.requires_grad=False\n",
    "    for p in model.cnn.parameters():\n",
    "        p.requires_grad=False\n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dicts(resnet):\n",
    "    return resnet.bn1.state_dict(), resnet.conv1.state_dict(), resnet.layers.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,c,cnn= get_state_dicts(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_model = rcnn(resnet, 4, b,c,cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 17, 140, 140]             459\n",
      "       BatchNorm2d-2         [-1, 17, 140, 140]              34\n",
      "            Conv2d-3           [-1, 34, 70, 70]           5,236\n",
      "       BatchNorm2d-4           [-1, 34, 70, 70]              68\n",
      "              ReLU-5           [-1, 34, 70, 70]               0\n",
      "            Conv2d-6           [-1, 34, 70, 70]          10,438\n",
      "       BatchNorm2d-7           [-1, 34, 70, 70]              68\n",
      "            Conv2d-8           [-1, 34, 70, 70]           5,236\n",
      "       ResNetBlock-9           [-1, 34, 70, 70]               0\n",
      "           Conv2d-10           [-1, 34, 70, 70]          10,438\n",
      "      BatchNorm2d-11           [-1, 34, 70, 70]              68\n",
      "             ReLU-12           [-1, 34, 70, 70]               0\n",
      "           Conv2d-13           [-1, 34, 70, 70]          10,438\n",
      "      BatchNorm2d-14           [-1, 34, 70, 70]              68\n",
      "      ResNetBlock-15           [-1, 34, 70, 70]               0\n",
      "           Conv2d-16           [-1, 34, 70, 70]          10,438\n",
      "      BatchNorm2d-17           [-1, 34, 70, 70]              68\n",
      "             ReLU-18           [-1, 34, 70, 70]               0\n",
      "           Conv2d-19           [-1, 34, 70, 70]          10,438\n",
      "      BatchNorm2d-20           [-1, 34, 70, 70]              68\n",
      "      ResNetBlock-21           [-1, 34, 70, 70]               0\n",
      "           Conv2d-22           [-1, 68, 35, 35]          20,876\n",
      "      BatchNorm2d-23           [-1, 68, 35, 35]             136\n",
      "             ReLU-24           [-1, 68, 35, 35]               0\n",
      "           Conv2d-25           [-1, 68, 35, 35]          41,684\n",
      "      BatchNorm2d-26           [-1, 68, 35, 35]             136\n",
      "           Conv2d-27           [-1, 68, 35, 35]          20,876\n",
      "      ResNetBlock-28           [-1, 68, 35, 35]               0\n",
      "           Conv2d-29           [-1, 68, 35, 35]          41,684\n",
      "      BatchNorm2d-30           [-1, 68, 35, 35]             136\n",
      "             ReLU-31           [-1, 68, 35, 35]               0\n",
      "           Conv2d-32           [-1, 68, 35, 35]          41,684\n",
      "      BatchNorm2d-33           [-1, 68, 35, 35]             136\n",
      "      ResNetBlock-34           [-1, 68, 35, 35]               0\n",
      "           Conv2d-35           [-1, 68, 35, 35]          41,684\n",
      "      BatchNorm2d-36           [-1, 68, 35, 35]             136\n",
      "             ReLU-37           [-1, 68, 35, 35]               0\n",
      "           Conv2d-38           [-1, 68, 35, 35]          41,684\n",
      "      BatchNorm2d-39           [-1, 68, 35, 35]             136\n",
      "      ResNetBlock-40           [-1, 68, 35, 35]               0\n",
      "           Conv2d-41          [-1, 136, 18, 18]          83,368\n",
      "      BatchNorm2d-42          [-1, 136, 18, 18]             272\n",
      "             ReLU-43          [-1, 136, 18, 18]               0\n",
      "           Conv2d-44          [-1, 136, 18, 18]         166,600\n",
      "      BatchNorm2d-45          [-1, 136, 18, 18]             272\n",
      "           Conv2d-46          [-1, 136, 18, 18]          83,368\n",
      "      ResNetBlock-47          [-1, 136, 18, 18]               0\n",
      "           Conv2d-48          [-1, 136, 18, 18]         166,600\n",
      "      BatchNorm2d-49          [-1, 136, 18, 18]             272\n",
      "             ReLU-50          [-1, 136, 18, 18]               0\n",
      "           Conv2d-51          [-1, 136, 18, 18]         166,600\n",
      "      BatchNorm2d-52          [-1, 136, 18, 18]             272\n",
      "      ResNetBlock-53          [-1, 136, 18, 18]               0\n",
      "           Conv2d-54            [-1, 272, 9, 9]         333,200\n",
      "      BatchNorm2d-55            [-1, 272, 9, 9]             544\n",
      "             ReLU-56            [-1, 272, 9, 9]               0\n",
      "           Conv2d-57            [-1, 272, 9, 9]         666,128\n",
      "      BatchNorm2d-58            [-1, 272, 9, 9]             544\n",
      "           Conv2d-59            [-1, 272, 9, 9]         333,200\n",
      "      ResNetBlock-60            [-1, 272, 9, 9]               0\n",
      "           Conv2d-61            [-1, 272, 9, 9]         666,128\n",
      "      BatchNorm2d-62            [-1, 272, 9, 9]             544\n",
      "             ReLU-63            [-1, 272, 9, 9]               0\n",
      "           Conv2d-64            [-1, 272, 9, 9]         666,128\n",
      "      BatchNorm2d-65            [-1, 272, 9, 9]             544\n",
      "      ResNetBlock-66            [-1, 272, 9, 9]               0\n",
      "           Linear-67                    [-1, 5]         110,165\n",
      "           Linear-68                    [-1, 4]          88,132\n",
      "================================================================\n",
      "Total params: 3,847,432\n",
      "Trainable params: 198,297\n",
      "Non-trainable params: 3,649,135\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.22\n",
      "Forward/backward pass size (MB): 47.86\n",
      "Params size (MB): 14.68\n",
      "Estimated Total Size (MB): 62.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(rcnn_model, (3,140,140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reste à définir la loss et le bon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_yolo(rect1, rect2, glob_im):\n",
    "    _,W,H = im.size()\n",
    "    x1, y1, w1, h1 = rect1 \n",
    "    x2, y2, w2, h2 = rect2\n",
    "    xa = x1 - w1/2\n",
    "    xb = x2 - w2/2\n",
    "    ya = y1 - h1/2\n",
    "    yb = y2 - h2/2\n",
    "    ha, hb = h1*H, h2*H\n",
    "    wa, wb = w1*W, w2*W\n",
    "    ra = [xa, ya, wa, ha]\n",
    "    rb = [xb, yb, wb, hb]\n",
    "    return IoU(ra,rb)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_positive(region, list_rec_lab, glob_im):\n",
    "    res = False\n",
    "    for _, rec in list_rec_lab:\n",
    "        i = iou_yolo(region, rec, glob_im)\n",
    "        if i>.5:\n",
    "            res=True\n",
    "            region_pos = rec\n",
    "            return res, rec\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_regions(img, list_rec_lab, region_prop):\n",
    "    #print(region_prop)\n",
    "    res = []\n",
    "    n_pos, n_neg = 0, 0\n",
    "    for region in region_prop:\n",
    "        isp = is_positive(region, list_rec_lab, img)\n",
    "        if isp==False and n_neg<96:\n",
    "            res.append((region, list_rec_lab[0][0]))\n",
    "        elif type(isp)==tuple:\n",
    "            _, rec = isp\n",
    "            res.append((region, list_rec_lab[0][0], rec))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_tensor(image, region):\n",
    "    x,y,w_r,h_r = region\n",
    "    \n",
    "    _,w,h = image.size()\n",
    "    x_abs ,y_abs = w*x ,h*y\n",
    "    y_top = y_abs - h_r*h/2\n",
    "    x_left = x_abs - w_r*w/2\n",
    "    return image[:, int(x_left):int(x_left+w_r*w), int(y_top):int(y_top+h_r*h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions(img_t):\n",
    "    #tr = torchvision.transforms.ToPILImage()\n",
    "    img_pil = img_t.permute(1,2,0).numpy()\n",
    "    _,W,H = img_t.size()\n",
    "    rects = selectivesearch(img_pil)\n",
    "    res = []\n",
    "    for rect in rects:\n",
    "        x,y,w,h = rect\n",
    "        x_c = (x + w/2)/W\n",
    "        y_c = (y + h/2)/H\n",
    "        w_r = w/W\n",
    "        h_r = h/H\n",
    "        res.append((x_c, y_c, w_r, h_r))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rcnn(model, optimizer, ds_detect_t, n_epoch = 40):\n",
    "    loss_clf = torch.nn.CrossEntropyLoss()\n",
    "    loss_reg = torch.nn.SmoothL1Loss()\n",
    "    for _ in tqdm.notebook.tqdm(range(n_epoch)):\n",
    "        for data in tqdm.notebook.tqdm(ds_detect_t):\n",
    "            optimizer.zero_grad()\n",
    "            image, list_rect_lab = data\n",
    "            region_prop = get_regions(image)\n",
    "            \n",
    "            regions = sample_regions(image, list_rect_lab, region_prop)\n",
    "            for reg in regions:\n",
    "                if len(reg)==2:\n",
    "                    # region negative \n",
    "                    region, label = reg\n",
    "                    ipt_net = crop_tensor(image, region)\n",
    "                    ipt_net = interpolate(ipt_net, 140)\n",
    "                    ipt_net = ipt_net.resize(1, 3, 140, 140)\n",
    "                    #return ipt_net\n",
    "                    clf, bbox = model(ipt_net)\n",
    "                    loss = loss_clf(clf, torch.tensor([4]))\n",
    "                    loss.backward()\n",
    "                else:\n",
    "                    # region positive\n",
    "                    region, label, gt_rect = reg\n",
    "                    ipt_net = crop_tensor(image, region)\n",
    "                    ipt_net = interpolate(ipt_net, 140)\n",
    "                    ipt_net = ipt_net.resize(1, 3, 140, 140)\n",
    "                    \n",
    "                    #return ipt_net\n",
    "                    \n",
    "                    clf, bbox = model(ipt_net)\n",
    "                    #print(gt_rect)\n",
    "                    loss = loss_reg(bbox, torch.tensor(gt_rect)) + loss_clf(clf, torch.tensor([label]))\n",
    "        \n",
    "                    loss.backward()      \n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rcnn_model\n",
    "#opt = torch.optim.Adam(rcnn_model.parameters())\n",
    "#rcnn_model = train_rcnn(rcnn_model, opt, ds_detect_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
